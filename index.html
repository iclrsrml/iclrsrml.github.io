<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="ICLR Workshop on Security in Machine Learning">

  <title>ICLR 2022 Workshop on Socially Responsible Machine Learning</title>

  <!-- Bootstrap core CSS -->
  <link href="bootstrap.min.css" rel="stylesheet">
</head>

<body>

<!-- Begin page content -->
<main role="main" class="container">
  <h1 class="mt-5 cneter">ICLR 2022 Workshop on Socially Responsible Machine Learning (SRML)
</h1>
  <p class="mb-0"><b>Date:</b> April 29, 2022 (Friday)</p>
  <p class="mb-0"><b>Location:</b> Virtual Only (co-located with <a href="https://iclr.cc/Conferences/2022/">ICLR 2022</a>)</p>
  <!-- <p> 
    <ul>
      <li>
        We've sent out the results for the papers. Check all the <a href="paper.html">accepted paper</a>.
      </li>
    </ul>
  </p> -->
  <!-- <p class="mb-0"><b>Contact:</b> xxx (this will email all organizers)</p> -->
  <p class="mb-0"><b>Abstract:</b></p>
  Machine learning (ML) systems have been increasingly used in many applications, ranging from decision-making systems (e.g., automated resume screening and pretrial release tool) to safety-critical tasks (e.g., face recognition, financial analytics and autonomous driving). Recently, the concept of foundation models has received significant attention in the ML community, which refers to the rise of models (e.g., BERT, GPT-3) that are trained on large-scale data and work surprisingly well in a wide range of downstream tasks. While there are many opportunities regarding foundation models, ranging from capabilities (e.g., language, vision, robotics, reasoning, human interaction), applications (e.g., law, healthcare, education, transportation), and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations), concerns and risks have been incurred that the models can inflict harm if they are not developed or used with care. It has been well-documented that ML models can:

    <p>
    <ul>
      <li>Inherit pre-existing social biases and exhibit discrimination against already-disadvantaged or marginalized social groups, such as BIPOC and LGBTQ+.</li>
      <li>Be vulnerable to security and/or privacy attacks that deceive the models and leak the sensitive information in training data, such as medical records and personal identifiable information (PII).</li>
      <li>Make hard-to-justify predictions with a lack of transparency and explanation.</li>
      <li>Be unreliable and unpredictable under domain shift or other input variations in real-life scenarios.</li>
    </ul>
    </p>
  <p>
      This workshop aims to build connections by bringing together both theoretical and applied researchers from various communities (e.g., machine learning, fairness &amp; ethics, security, privacy, etc.), with a focus on recent research and future directions for socially responsible machine learning problems in real-world systems.
    
  </p>
  <p>
  <!--For example, various commercial face recognition products were shown to have racial/gender bias, with female and darker-skinned people experiencing a higher chance of classification error. Additionally, in domains such as financial analytics and autonomous vehicles, ML models could be easily misled by carefully-crafted small perturbations or even simple out-of-distribution natural examples. Therefore, it is essential to build socially responsible ML models that are fair, robust, private, transparent, and interpretable. -->
     <!-- Finally, we hope to chart out important directions for future work and cross-community collaborations, including computer vision, machine learning, security, and multimedia communities. -->
  </p>
  <!-- <h2>Sponsor</h2> -->
  <!-- <p></p> -->

<h2>Important Dates</h2>
<ul>
  <li><b>Workshop paper submission deadline:</b> 02/25/2022</li>
  <li><b>Notification to authors:</b> 03/25/2022</li>
  <li><b>Camera ready deadline:</b> 04/10/2022</li>
  <li><b>Workshop day:</b> 04/29/2022</li>
</ul>
	

  <h2>Schedule</h2>
  <p><b>Workshop day:</b> 04/29/2022</p>
  <p><b>Time Zone:</b> US Eastern Time Zone (UTC-05:00)  </p>
  <p>Detailed schedule will be released soon once we confirm the availability of all speakers.</p>
  <!-- <p>The tentative schedule is subject to change prior to the workshop.</p> -->
  <!-- <p>Accepted paper can be found <a href="paper.html">here</a>.</p> -->
  <h2>Organizing Committee</h2>
  <div class="row justify-content-around">
    <!-- <div class="col-lg-1"></div> -->
    <!-- </div> -->
      <div class="col-md-1">
      <img class="rounded-circle" src="imgs/chaowei.jpeg" width="150px" height="150px">
      <p style="width:150px;text-align:center; text-align:center" >Chaowei Xiao</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/xueru.png" width="150px" height="150px">
      <p style="width:150px;text-align:center" >Xueru Zhang</p>
    </div>  
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/huanzhang.jpeg" width="150px" height="150px">
      <p style="width:150px;text-align:center;text-align:center" >Huan Zhang</p>
    </div>

    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/hongyang.png" width="150px" height="150px">
      <p style="width:150px;text-align:center;text-align:center" >Hongyang Zhang  </p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/cihang.jpeg" width="150px" height="150px">
      <p style="width:150px;text-align:center" >Cihang Xie</p>
    </div>


  <div class="col-md-1">
    <img class="rounded-circle" src="imgs/xinchen.png" width="150px" height="150px">
    <p style="width:150px;text-align:center" >Xinchen Yan</p>
  </div>
</div>
  <div class="row justify-content-around">
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/beidi.png" width="150px" height="150px">
      <p style="width:150px;text-align:center" >Beidi Chen</p>
    </div>

  <div class="col-md-1">
    <img class="rounded-circle" src="imgs/yuke.png" width="150px" height="150px">
    <p style="width:150px;text-align:center" >Yuke Zhu</p>
  </div>
  <!-- <h2>Senior Organizing Committee</h2> -->
    <!-- <div class="row justify-content-around"> -->
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/boli.png" width="150px" height="150px">
      <p style="width:150px;text-align:center" >Bo Li<br /></p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/zico.jpeg" width="150px" height="150px">
      <p style="width:150px;text-align:center" >Zico Kolter</p>
    </div>

    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/dawn.png" width="150px" height="150px">
      <p style="width:150px;text-align:center" >Dawn Song</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/anima.png" width="150px" height="150px">
      <p style="width:150px;text-align:center" >Anima Anandkumar</p>
    </div>
    <!-- <div class="col-lg-1"></div> -->
  </div>

<!--   
<h2>Program Committee</h2>
<li>Aishan Liu (Beihang University)</li>
<li>Anqi Liu (Caltech)</li>
<li>Akshayvarun	Subramanya (UMBC)</li>
<li>Alexandra	 Chouldechova (CMU)</li>
<li>Aniruddha		Saha(University of Maryland Baltimore County)</li>
<li>Anshuman		Suri (University of Virginia)</li>
<li>Bo Ji (Virginia Tech)</li>
<li>Boxin		Wang (University of Illinois at Urbana-Champaign)</li>
<li>Chen		Zhu (University of Maryland)</li>
<li>Chirag		Agarwal	(Harvard University)</li>
<li>Chulin		Xie	(University of Illinois at Urbana-Champaign) </li>
<li>Hongyang		Zhang	(TTIC)</li>
<li>Huan		Zhang	(UCLA) </li>
<li>Jamie		Hayes (Google DeepMind)	</li>
<li>Jia		Liu	(Ohio State University)	</li>
<li>Jiachen		Sun	(University of Michigan) </li>
<li>Josiah		Wong (Stanford University)	</li>	
<li>Juba		Ziani (University of Pennsylvania)</li>	
<li>Junheng		Hao		(UCLA)	</li>	
<li>Kexin		Rong	 	(Stanford University)	</li>	
<li>Kun		Jin	 (University of Michigan, Ann Arbor)</li>	
<li>Maura		Pintor 	(University of Cagliari)</li>		
<li>Mohammad Mahdi	 	Khalili	 	(University of Delaware)</li>	
<li>Muhammad		Awais	 (Kyung-Hee University)</li>	
<li>Nataniel		Ruiz	 	(Boston University)</li>	
<li>Parinaz		Naghizadeh 	(Ohio State University)</li>		
<li>Rajkumar		Theagarajan	 	(University of California, Riverside)</li>	
<li>Sravanti		Addepalli	 	(Indian Institute of Science)</li>	
<li>Sunipa		Dev	 	(University of Utah)</li>	
<li>Wenxiao		Wang	 	(Tsinghua University)</li>		
<li>Won		Park	 (University of Michigan)</li>	
<li>Xinchen		Yan	 (Uber ATG)</li>	
<li>Xingjun		Ma	 (Deakin University)	</li>	
<li>Xinlei		Pan	 	(UC Berkeley)</li>	
<li>Xinwei		Zhao	 	(Drexel University)	</li>	
<li>Xueru	 	Zhang	 (University of Michigan)</li>	
<li>Yingwei		Li	 	(Johns Hopkins University)</li>	
<li>Yizhou	Sun	 	(UCLA)</li>	
<li>Yulong		Cao	 (University of Michigan, Ann Arbor) </li>	
<li>Yuzhe		Yang	 (MIT)</li>	
<li>Zelun		Luo	 	(Stanford University)</li>	
<li>Zhiding		Yu	 	(NVIDIA)</li>	 -->

<!-- <h2 id="accepted">Accepted Paper</h2> -->


 
<h2>Call For Papers</h2>

Although extensive studies have been conducted to increase trust in ML, many of them either focus on well-defined problems that enable nice tractability from a mathematical perspective but are hard to be adapted to real-world systems, or they mainly focus on mitigating risks in real-world applications without providing theoretical justifications. Moreover, most work studies fairness, privacy, transparency, and interpretability separately, while the connections among them are less explored. This workshop aims to bring together both theoretical and applied researchers from various communities (e.g., machine learning, fairness &amp; ethics, security, privacy, etc.). <b>We invite submissions on any aspect of the social responsibility and trustworthiness of machine learning, which includes but not limited to</b>:

<p>
<ul>
    <li>The intersection of various aspects of socially responsible and trustworthy ML: fairness, transparency, interpretability, privacy, robustness.</li>
    <li>The state-of-the-art research of socially responsible and trustworthy ML and their usage in applications.</li>
    <li>The possibility of using the most recent theoretical advancements to inform practice guidelines for deploying socially responsible and trustworthy ML systems.</li>
    <li>Providing insights about how we can automatically detect, verify, explain, and mitigate potential biases, privacy or other societal problems in existing models.</li>
    <li>Understanding the trade-offs or costs of achieving different goals in reality.</li>
    <li>Studying the social impacts of machine learning models that may inherently have bias, exhibit discrimination, or cause other undesired harms.</li>
</ul>
</p>
<p>
Reviewing will be performed in double-blind, with criteria include (a) relevance, (b) quality of the methodology and experiments, (c) novelty, (d) societal impacts.
<p>
<p>
  <p class="mb-0"><b>Submission deadline:</b> Feb 25, 2022 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Notification sent to authors:</b> Mar 25, 2022 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Submission server:</b> <a href="https://cmt3.research.microsoft.com/ICLRSRML2022/" target="_blank">https://cmt3.research.microsoft.com/ICLRSRML2022/</a></p>
	
   <p class="mb-0"><b>Submission Format: </b> We welcome submissions up to 4 pages in ICLR Proceedings format (double-blind), excluding references and appendix. <a href="https://github.com/ICLR/Master-Template/raw/master/archive/iclr2022.zip">Style files</a>  are available.   We allow an unlimited number of pages for references and supplementary material, but reviewers are not required to review the supplementary material.  Unless indicated by the authors, we will provide PDFs of all accepted papers on https://iclrsrml.github.io/.  There will be no archival proceedings.
  
    We are using CMT3 to manage submissions.
    </p>
    <p class="mb-0"><b>Contact: </b> Please email <a href="https://xiaocw11.github.io/">Chaowei Xiao</a> (xiaocw [at] umich [dot] edu) and <a href="https://huan-zhang.com/">Huan Zhang</a> (huan [at] huan-zhang [dot] com) for submission issues.
    </p>
    <!-- <p class="mb-0"><b>Ethics: </b> We ask that authors think about the broader impact and ethical considerations of their work. For example, authors may consider whether there is potential use for the data or methods to create or exacerbate unfair bias. Authors are not required to have a broader impact statement in their paper, but will be asked to submit one paragraph (3-4 sentences) as a separate field in OpenReview at the time of submission. See this guide for help with the statement. Reviewers will be asked to consult the ICLR 2021 code of ethics when reviewing submissions. Reviewers cannot reject papers based on ethical considerations, but in very rare cases, the workshop organizers may reject papers that blatantly violate the code of ethics (e.g., if the primary application directly causes harm or injury). 
    </p> -->
    
    
    
   </p>

</body></html>
