<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="ICLR Workshop on Security in Machine Learning">

  <title>ICLR 2022 Workshop on Socially Responsible Machine Learning</title>

  <!-- Bootstrap core CSS -->
  <link href="bootstrap.min.css" rel="stylesheet">
</head>

<body>

<!-- Begin page content -->
<main role="main" class="container">
  <h1 class="mt-5 cneter">ICLR 2022 Workshop on Socially Responsible Machine Learning (SRML)
</h1>
  <p class="mb-0"><b>Date:</b> April 29, 2022 (Friday)</p>
  <p class="mb-0"><b>Location:</b> Virtual Only (co-located with <a href="https://iclr.cc/Conferences/2022/">ICLR 2022</a>)</p>
  <!-- <p> 
    <ul>
      <li>
        We've sent out the results for the papers. Check all the <a href="paper.html">accepted paper</a>.
      </li>
    </ul>
  </p> -->
  <!-- <p class="mb-0"><b>Contact:</b> xxx (this will email all organizers)</p> -->
  <p class="mb-0"><b>Abstract:</b></p>
  Machine learning (ML) systems have been increasingly used in many applications, ranging from decision-making systems (e.g., automated resume screening and pretrial release tool) to safety-critical tasks (e.g., face recognition, financial analytics and autonomous driving). Recently, the concept of foundation models has received significant attention in the ML community, which refers to the rise of models (e.g., BERT, GPT-3) that are trained on large-scale data and work surprisingly well in a wide range of downstream tasks. While there are many opportunities regarding foundation models, ranging from capabilities (e.g., language, vision, robotics, reasoning, human interaction), applications (e.g., law, healthcare, education, transportation), and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations), concerns and risks have been incurred that the models can inflict harm if they are not developed or used with care. It has been well-documented that ML models can:

    <p>
    <ul>
      <li>Inherit pre-existing social biases and exhibit discrimination against already-disadvantaged or marginalized social groups, such as BIPOC and LGBTQ+.</li>
      <li>Be vulnerable to security and/or privacy attacks that deceive the models and leak the sensitive information in training data, such as medical records and personal identifiable information (PII).</li>
      <li>Make hard-to-justify predictions with a lack of transparency and explanation.</li>
      <li>Be unreliable and unpredictable under domain shift or other input variations in real-life scenarios.</li>
    </ul>
    </p>
  <p>
      This workshop aims to build connections by bringing together both theoretical and applied researchers from various communities (e.g., machine learning, fairness &amp; ethics, security, privacy, etc.), with a focus on recent research and future directions for socially responsible machine learning problems in real-world systems.
    
  </p>


<h2 id="accepted">Meeting link</h2>
You could click this <a href="https://us06web.zoom.us/j/88085635354?pwd=VjJDQi96VFFUUzI0OE5WTS9vZlZHQT09">zoom link </a> to join the workshop. 

<h2 id="accepted">Accepted Papers</h2>

Our workshop accepted <b>28 high quality papers</b>: <a href="paper.html">List of accepted papers</a>

<p></p>

<h2>Schedule</h2>
  <p><b>Workshop day:</b> 04/29/2022</p>
  <p><b>Time Zone:</b> US Eastern Time Zone (UTC-05:00)  </p>
  <table class="table table-sm">
    <tbody>
    <tr>
      <th scope="row">8:45-9:00</th>
      <td>Opening Remarks (Prof. Bo Li)</td>
      <td></td>
    </tr>

    <!-- <tr><th scope="row" colspan="3">Session 1:Interpretable Machine Learning Models</th></tr> -->
    <tr>
      <th scope="row">9:00-9:40</th>
      <td> Invited talk from Prof. Olga Russakovsky  </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">9:40-10:20</th>
      <td> Invited talk from Prof. Ziwei Liu </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">10:20-11:00</th>
      <td> Invited talk from Prof. Aleksander Mądry</td>
      <td></td>
    </tr>
    <!-- <tr>
      <th scope="row">10:40 - 11:00</th>
      <td>Contributed Talk #2: Auditing AI models for Verified Deployment under Semantic Specifications </td>
    </tr> -->
    <!-- <tr>
      <th scope="row">10:20 - 10:40</th>
      <td>Contributed Talk #1: FERMI: Fair Empirical Risk Minimization Via Exponential Rényi Mutual Information </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">10:40 - 11:00</th>
      <td>Contributed Talk #2: Auditing AI models for Verified Deployment under Semantic Specifications </td>
    </tr> -->
    <tr><th scope="row" colspan="3">Coffee Break</th></tr>
    <!-- <tr><th scope="row" colspan="3">Session 2:Adversarial Examples in Physical World </th></tr> -->
    <tr>
      <th scope="row">11:10-11:50</th>
      <td> Invited talk from Prof. Anqi Liu </td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">11:50-12:30</th>
      <td>  Invited talk from Prof. Judy Hoffman     </td>
      <td></td>
    </tr>
    <tr><th scope="row" colspan="3"> Lunch </th></tr>
    <tr>
      <th scope="row">13:30-13:50</th>
      <td>Contributed Talk #1: <a href="https://download.huan-zhang.com/events/srml2022/accepted/sun22modelnet40c.pdf">Modelnet40-c: a robustness benchmark for 3d point cloud recognition under corruption      </a></td>
      <td></td>
    <tr>
      <th scope="row">13:50-14:30</th>
      <td> Invited talk from Neil Gong  </td>
      <td></td>
    </tr>
    </tr>
    <tr>
      <th scope="row">14:30-15:10</th>
      <td>Invited talk from Virginia Smith </td>
    </tr>
    <tr><th scope="row" colspan="3">Break</th></tr>
    
    <tr>
      <th scope="row">15:20-16:00</th>
      <td> Invited talk from Prof. Marco Pavone and Dr. Apoorva Sharma </td>
      <td><a target="_blank"></a></td>
    </tr>
    <tr>
      <th scope="row">16:00-16:40 </th>
      <td>Invited talk from Prof Diyi Yang </td>
      <td></td>
    </tr>

    <tr>
      <th scope="row">16:40-17:00</th>
      <td>Contributed Talk #2: <a href="https://download.huan-zhang.com/events/srml2022/accepted/hu22provably.pdf">Provably fair federated learning via bounded group loss</a></td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">17:00-17:20</th>
      <td>Contributed Talk #3: <a href="https://download.huan-zhang.com/events/srml2022/accepted/marcinkevics22debiasing.pdf">Debiasing neural networks using differentiable classification parity proxies</a></td>
      <td></td>
    </tr>
    <tr>
      <th scope="row">17:20-17:40</th>
      <td>Contributed Talk #4: <a href="https://download.huan-zhang.com/events/srml2022/accepted/balcan22can.pdf">Can non-lipschitz networks be robust? the power of abstention and data-driven decision making for robust non-lipschitz networks</a></td>
    </tr>
    <tr>
      <th scope="row">17:40-18:00</th>
      <td>Contributed Talk #5: <a href="https://download.huan-zhang.com/events/srml2022/accepted/angrisani22differential.pdf">    Differential Privacy Amplification in Quantum and Quantum-inspired Algorithms 
      </a></td>
    </tr>

    <tr>
      <th scope="row">18:00-18:20</th>
      <td>Contributed Talk #6: <a href="https://download.huan-zhang.com/events/srml2022/accepted/jin22incentive.pdf">Incentive Mechanisms in Strategic Learning</a></td>
    </tr>
    <tr>
      <th scope="row">19:00-20:00</th>
      <td>Gathertown Poster Session. (link:<a href="https://gather.town/oWwNGOinBf9nm2gR/iclr2022srml"> https://gather.town/oWwNGOinBf9nm2gR/iclr2022srml</a>) </td>
    </tr>
    </tbody>
</table>  
 

<h2>Invited Speakers</h2>
<div class="row justify-content-around">
  <!-- <div class="col-lg-1"></div> -->
  <!-- </div> -->
    <div class="col-md-1">
    <img class="rounded-circle" src="imgs/spk/olga.jpeg" width="150px" height="150px">
    <p style="width:150px;text-align:center;" ><a href="https://www.cs.princeton.edu/~olgarus/"><b>Olga Russakovsky</b></a><br>
    (Princeton)</p>
  </div>

  <div class="col-md-1">
    <img class="rounded-circle" src="imgs/spk/ziwei.png" width="150px" height="150px">
    <p style="width:150px;text-align:center;" ><a href="https://liuziwei7.github.io/"><b>Ziwei Liu</b></a><br>
    (Nanyang Technological University)</p>
  </div>
  <div class="col-md-1">
    <img class="rounded-circle" src="imgs/spk/madry.png" width="150px" height="150px">
    <p style="width:150px;text-align:center;" ><a href="http://madry-lab.ml/"><b>Aleksander Mądry</b></a><br>
    (MIT)</p>
  </div>  
  <div class="col-md-1">
    <img class="rounded-circle" src="imgs/spk/anqi.png" width="150px" height="150px">
    <p style="width:150px;text-align:center;" ><a href="https://anqiliu-ai.github.io/"><b>Anqi (Angie) Liu</b></a><br>
    (Johns Hopkins University)</p>
  </div>
  <div class="col-md-1">
    <img class="rounded-circle" src="imgs/spk/judy.jpeg" width="150px" height="150px">
    <p style="width:150px;text-align:center;" ><a href="https://faculty.cc.gatech.edu/~judy/"><b>Judy Hoffman</b></a><br>
    (Georgia Tech)</p>
  </div>
</div> 
<div class="row justify-content-around">
<div class="col-md-1">
  <img class="rounded-circle" src="imgs/spk/neil.jpg" width="150px" height="150px">
  <p style="width:150px;text-align:center;" ><a href="https://people.duke.edu/~zg70/"><b>Neil Gong</b></a><br>
  (Duke University)</p>
</div>
<div class="col-md-1">
  <img class="rounded-circle" src="imgs/spk/virginia.png" width="150px" height="150px">
    <p style="width:150px;text-align:center;" ><a href="https://www.cs.cmu.edu/~smithv/"><b>Virginia Smith</b></a><br>
    (CMU)
  </p>
</div>
<div class="col-md-1">
  <img class="rounded-circle" src="imgs/spk/pavone.jpeg" width="150px" height="150px">
  <p style="width:150px;text-align:center;" ><a href="https://web.stanford.edu/~pavone/"><b>Marco Pavone</b></a><br>
  (Stanford)</p>
</div>
  <div class="col-md-1">
    <img class="rounded-circle" src="imgs/spk/diyi.jpg" width="150px" height="150px">
    <p style="width:150px;text-align:center;" ><a href="https://faculty.cc.gatech.edu/~dyang888/"><b>Diyi Yang</b></a><br>
    (Georgia Tech)</p>
  </div>
</div>
<p></p>

  <h2>Organizing Committee</h2>
  <div class="row justify-content-around">
    <!-- <div class="col-lg-1"></div> -->
    <!-- </div> -->
      <div class="col-md-1">
      <img class="rounded-circle" src="imgs/chaowei.jpeg" width="150px" height="150px">
      <p style="width:150px;text-align:center;" ><a href="https://xiaocw11.github.io/"><b>Chaowei Xiao</b></a><br>
      (NVIDIA)</p>
    </div>

    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/huanzhang.jpeg" width="150px" height="150px">
      <p style="width:150px;text-align:center;" ><a href="https://huan-zhang.com"><b>Huan Zhang</b></a><br>
      (CMU)</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/xueru.png" width="150px" height="150px">
      <p style="width:150px;text-align:center;" ><a href="https://xueruzhang.github.io/"><b>Xueru Zhang</b></a><br>
      (Ohio State University)</p>
    </div>  
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/hongyang.png" width="150px" height="150px">
      <p style="width:150px;text-align:center;" ><a href="https://hongyanz.github.io/"><b>Hongyang Zhang</b></a><br>
      (Waterloo)</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/cihang.jpeg" width="150px" height="150px">
      <p style="width:150px;text-align:center;" ><a href="https://cihangxie.github.io/"><b>Cihang Xie</b></a><br>
      (UCSC)</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/beidi.png" width="150px" height="150px">
      <p style="width:150px;text-align:center;" ><a href="https://cs.stanford.edu/people/beidic/"><b>Beidi Chen</b></a><br>
      (Stanford)</p>
    </div>
  </div> 
  <div class="row justify-content-around">
  <div class="col-md-1">
    <img class="rounded-circle" src="imgs/xinchen.png" width="150px" height="150px">
      <p style="width:150px;text-align:center;" ><a href="https://sites.google.com/site/skywalkeryxc/"><b>Xinchen Yan</b></a><br>
    </p>
  </div>
<!-- </div> -->
  <div class="col-md-1">
    <img class="rounded-circle" src="imgs/yuke.png" width="150px" height="150px">
    <p style="width:150px;text-align:center;" ><a href="https://rpl.cs.utexas.edu/"><b>Yuke Zhu</b></a><br>
    (UT Austin/NVIDIA)</p>
  </div>
  <!-- <h2>Senior Organizing Committee</h2> -->
    <!-- <div class="row justify-content-around"> -->
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/boli.png" width="150px" height="150px">
      <p style="width:150px;text-align:center;" ><a href="https://aisecure.github.io/"><b>Bo Li</b></a><br>
      (UIUC)</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/zico.jpeg" width="150px" height="150px">
      <p style="width:150px;text-align:center;" ><a href="https://zicokolter.com/"><b>Zico Kolter</b></a><br>
      (CMU)</p>
    </div>

    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/dawn.png" width="150px" height="150px">
      <p style="width:150px;text-align:center;" ><a href="https://people.eecs.berkeley.edu/~dawnsong/"><b>Dawn Song</b></a><br>
      (UC Berkeley)</p>
    </div>
    <div class="col-md-1">
      <img class="rounded-circle" src="imgs/anima.png" width="150px" height="150px">
      <p style="width:150px;text-align:center;" ><a href="http://tensorlab.cms.caltech.edu/users/anima/"><b>Anima Anandkumar</b></a><br>
      (Caltech/NVIDIA)</p>
    </div>
    <!-- <div class="col-lg-1"></div> -->
  </div>
  <p></p>

<h2>Program Committee</h2>

<li> Chirag Agarwal (Adobe) </li>
<li> Yulong Cao (University of Michigan, Ann Arbor) </li>
<li> Junheng Hao (UCLA) </li>
<li> Lifeng Huang (SunYat-sen university) </li>
<li> Kun Jin (University of Michigan, Ann Arbor) </li>
<li> Mohammad Mahdi Khalili (University of Delaware) </li>
<li> Adam Kortylewski (Max Planck Institute for Informatics) </li>
<li> Jia Liu (The Ohio State University) </li>
<li> Xingjun Ma (Deakin University) </li>
<li> Parinaz Naghizadeh (Ohio State University) </li>
<li> Xinlei Pan (UC Berkeley) </li>
<li> Swetasudha Panda (Oracle Labs) </li>
<li> Won Park (University of Michigan) </li>
<li> Maura Pintor (University of Cagliari) </li>
<li> Nataniel Ruiz (Boston University) </li>
<li> Aniruddha Saha (University of Maryland Baltimore County) </li>
<li> Gaurang Sriramanan (University of Maryland, College Park) </li>
<li> Akshayvarun Subramanya (UMBC) </li>
<li> Jiachen Sun (University of Michigan) </li>
<li> Anshuman Suri (University of Virginia) </li>
<li> Rajkumar Theagarajan (University of California, Riverside) </li>
<li> Chang Xiao (Columbia University) </li>
<li> Chulin Xie (University of Illinois at Urbana-Champaign) </li>
<li> Xinchen Yan (Waymo) </li>
<li> Hongyang Zhang (University of Waterloo) </li>
<li> Xinwei Zhao (Drexel University) </li>


<p></p>
 
<h2>Important Dates</h2>
<ul>
  <li><b>Workshop paper submission deadline:</b> 02/25/2022 (<a href="https://www.timeanddate.com/time/zones/aoe">AoE</a>)</li>
  <li><b>Notification to authors:</b> 03/25/2022</li>
  <li><b>Camera ready deadline:</b> 04/10/2022</li>
  <li><b>Workshop day:</b> 04/29/2022</li>
</ul>
  <p></p>
	
<h2>Call For Papers</h2>

Although extensive studies have been conducted to increase trust in ML, many of them either focus on well-defined problems that enable nice tractability from a mathematical perspective but are hard to be adapted to real-world systems, or they mainly focus on mitigating risks in real-world applications without providing theoretical justifications. Moreover, most work studies fairness, privacy, transparency, and interpretability separately, while the connections among them are less explored. This workshop aims to bring together both theoretical and applied researchers from various communities (e.g., machine learning, fairness &amp; ethics, security, privacy, etc.). <b>We invite submissions on any aspect of the social responsibility and trustworthiness of machine learning, which includes but not limited to</b>:

<p>
<ul>
    <li>The intersection of various aspects of socially responsible and trustworthy ML: fairness, transparency, interpretability, privacy, robustness.</li>
    <li>The state-of-the-art research of socially responsible and trustworthy ML and their usage in applications.</li>
    <li>The possibility of using the most recent theoretical advancements to inform practice guidelines for deploying socially responsible and trustworthy ML systems.</li>
    <li>Providing insights about how we can automatically detect, verify, explain, and mitigate potential biases, privacy or other societal problems in existing models.</li>
    <li>Understanding the trade-offs or costs of achieving different goals in reality.</li>
    <li>Studying the social impacts of machine learning models that may inherently have bias, exhibit discrimination, or cause other undesired harms.</li>
</ul>
</p>
<p>
Reviewing will be performed in double-blind, with criteria include (a) relevance, (b) quality of the methodology and experiments, (c) novelty, (d) societal impacts.
<p>
<p>
  <p class="mb-0"><b>Poster deadline:</b> April 17, 2022 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Camera-ready deadline:</b> April 10, 2022 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Submission deadline:</b> Feb 25, 2022 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Notification sent to authors:</b> Mar 25, 2022 Anywhere on Earth (AoE)</p>
  <p class="mb-0"><b>Submission server:</b> <a href="https://cmt3.research.microsoft.com/ICLRSRML2022/" target="_blank">https://cmt3.research.microsoft.com/ICLRSRML2022/</a></p>
	
  <p class="mb-0"><b>Submission Format: </b> We welcome submissions up to 4 pages in ICLR Proceedings format (double-blind), excluding references and appendix. <a href="https://github.com/ICLR/Master-Template/raw/master/archive/iclr2022.zip">Style files</a>  are available.   We allow an unlimited number of pages for references and supplementary material, but reviewers are not required to review the supplementary material.  Unless indicated by the authors, we will provide PDFs of all accepted papers on <a href="https://iclrsrml.github.io/">https://iclrsrml.github.io/</a>.  There will be no archival proceedings.
  
    We are using CMT3 to manage submissions.
    </p>
    <p class="mb-0"><b>Contact: </b> Please email <a href="https://xiaocw11.github.io/">Chaowei Xiao</a> (xiaocw [at] umich [dot] edu) and <a href="https://huan-zhang.com/">Huan Zhang</a> (huan [at] huan-zhang [dot] com) for submission issues.
    </p>
    <!-- <p class="mb-0"><b>Ethics: </b> We ask that authors think about the broader impact and ethical considerations of their work. For example, authors may consider whether there is potential use for the data or methods to create or exacerbate unfair bias. Authors are not required to have a broader impact statement in their paper, but will be asked to submit one paragraph (3-4 sentences) as a separate field in OpenReview at the time of submission. See this guide for help with the statement. Reviewers will be asked to consult the ICLR 2021 code of ethics when reviewing submissions. Reviewers cannot reject papers based on ethical considerations, but in very rare cases, the workshop organizers may reject papers that blatantly violate the code of ethics (e.g., if the primary application directly causes harm or injury). 
    </p> -->
    
    
    
   </p>
  <p></p>

</body></html>
